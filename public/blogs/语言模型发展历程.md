---
title: 语言模型发展历程
date: 2025-02-03
readTime: 4 min read
category: 人工智能
---

## 为什么 ChatGPT 3.5 会在2022年11月底发布后爆火？

从技术的角度有一下三点原因：

1. 模型能力的跃升
ChatGPT基于GPT-3.5架构，引入了**强化学习与人类反馈（RLHF）**技术，显著提升了对话连贯性、逻辑性和安全性。相比GPT-3，它更擅长处理多轮对话、修正错误回答，并减少生成有害内容的能力。
2. 用户交互体验优化
通过对话式界面，ChatGPT降低了AI技术的使用门槛。用户无需编程知识即可直接体验，满足了普通人对AI的直观认知——"像人类一样聊天"。
3. 免费开放测试策略
OpenAI在2022年11月30日向公众开放免费试用，允许用户零成本体验强大的生成能力，迅速积累大规模用户基础。

![llm_history](/blogs/images/llm_history.png)

## 传统语言模型

### 统计语言模型

#### 链式法则

链式法则，也就是条件概率的乘积。比如，一个句子由多个词组成，每个词的概率依赖于其前面所有词的历史。

```
P(w₁, w₂, ..., wₙ) = P(w₁) · P(w₂|w₁) · P(w₃|w₁, w₂) ... P(wₙ|w₁, ..., wₙ₋₁)
```

缺点：直接计算长序列的复杂度极高。

#### 马尔可夫假设与n-gram模型

为简化计算，引入马尔可夫假设，限定当前词仅依赖前k个词（通常k=1或2），形成n-gram模型：
- **Bigram**（二元模型）：`P(wᵢ|wᵢ₋₁)`
- **Trigram**（三元模型）：`P(wᵢ|wᵢ₋₂, wᵢ₋₁)`
参数数量大幅减少，但牺牲了长距离依赖的捕捉能力。

**参数估计与最大似然**：
 通过统计语料库中的词频估计条件概率。例如，Bigram概率：
 ```
 P(wᵢ|wᵢ₋₁) = count(wᵢ₋₁, wᵢ) / count(wᵢ₋₁)
 ```
 依赖大量数据以确保统计显著性。

##### 统计语言模型对数据稀疏性的处理

**平滑技术**：
 解决零概率问题（未登录n-gram）的常见方法包括：
 - **拉普拉斯平滑**：为所有n-gram计数加1。
 - **Good-Turing估计**：重新分配低频事件的概率。
 - **Kneser-Ney平滑**：考虑词的上下文多样性，尤其适用于高阶n-gram。

**对数概率与计算优化**：
 为避免数值下溢，将概率乘积转换为对数概率求和：
 ```
 log P(w₁, ..., wₙ) = Σᵢ₌₁ⁿ log P(wᵢ|wᵢ₋ₖ, ..., wᵢ₋₁)
 ```

统计语言模型以概率论为核心，通过链式分解和马尔可夫假设简化计算，结合统计估计与平滑技术处理数据稀疏性，为自然语言处理提供了基础框架。

### 神经语言模型

神经语言模型（Neural Language Model, NLM）的核心原理是通过神经网络对自然语言的概率分布进行建模，以预测序列中下一个词（或字符）的概率。其核心思想是用神经网络捕捉语言的统计规律和语义特征，从而生成连贯且符合语境的文本。

早期工作 (MLP) : 单词映射到词向量，再由神经网络预测当前时刻词汇

#### 循环神经网络（RNN）

* 循环结构：通过隐藏状态（Hidden State）逐步处理序列，每一步的输出依赖于当前输入和前一步的隐藏状态。
* 时序依赖：严格按时间步顺序处理序列（从左到右或双向），无法并行计算。
* 典型变体：LSTM（长短期记忆网络）、GRU（门控循环单元），通过门控机制缓解梯度消失问题。
  
RNN 更适合短序列、低资源或实时性要求高的任务，但受限于长距离依赖和训练效率。

#### transformer

* 自注意力机制：通过自注意力（Self-Attention）直接计算序列中所有位置之间的关系，无需逐步处理。
* 并行计算：所有位置的输入同时处理，天然支持并行化。
* 位置编码：通过位置嵌入（Positional Encoding）显式引入位置信息，弥补注意力机制对顺序不敏感的缺陷。

Transformer 凭借全局注意力、并行计算和大规模预训练能力，成为当前NLP的主流架构，但需要更多计算资源。

#### Word2Vec

Word2Vec 是一种基于神经网络的词嵌入（Word Embedding）技术，旨在将自然语言中的词语映射为低维稠密向量（词向量），从而捕捉词语之间的语义和语法关系。其核心原理是通过词语的上下文信息来学习词向量，使得语义相近的词语在向量空间中距离较近。

### 预训练语言模型

预训练语言模型（Pre-trained Language Models, PLMs）可以通过大规模无监督学习捕捉语言的内在规律和知识，并将这些知识迁移到下游任务中。

预训练语言模型的强大之处在于：当模型规模达到一定程度时，可能自发出现零样本学习、逻辑推理、代码生成等复杂能力，这些能力并非显式设计，而是从数据中隐式习得。（模型在没有刻意训练的情况下，掌握了一定的泛化能力和推理能力）

#### 自监督学习（Self-supervised Learning）
核心任务设计：模型通过设计“预测任务”从无标注文本中自动生成监督信号，例如：

掩码语言建模（Masked Language Modeling, MLM）：如BERT，随机遮盖输入文本中的部分词，让模型预测被遮盖的词。（完形填空）

自回归语言建模（Autoregressive LM）：如GPT，让模型根据上文预测下一个词（从左到右生成）。

### 传统语言模型的局限性

我们把统计语言模型、神经语言模型、预训练语言模型统一成为传统语言模型。

1. 缺乏背景知识
2. 任务泛化性较差
3. 复杂推理能力较弱

大语言模型的出现同时解决了上述问题

## 大语言模型

### 特点

* 模型参数大（数十亿、百亿、千亿甚至万亿）
* 用于预训练的数据规模大
* 与传统语言模型相比，需要更复杂、精细的模型训练方法

### 为什么模型需要有很多参数

### 为什么模型需要大规模的数据用来预训练

* 数据规模越大，模型学习到的能力就越强
* 给模型提供大规模数据，会提升模型的泛化能力

> 数据数量、数据质量决定了模型的能力，同样意味着大算力的需求
